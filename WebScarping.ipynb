{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSJ7JZId_BYE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import bs4 as beautifulsoap\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The original url\n",
        "url = \"http://lhr.nu.edu.pk/faculty/\"\n",
        "# making a request for geting the url\n",
        "request = requests.get(url)\n",
        "try:\n",
        "  request.raise_for_status()\n",
        "  # checking if the url gives successful http codes\n",
        "  print(\"Working on it!\")\n",
        "  print(\"Successfully opened the URL!\")\n",
        "# requesting exception in the form of m and then displaying it as an error\n",
        "except requests.exceptions.RequestException as m:\n",
        "  print(f\"Error while opening url! {m}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwqAHrDZ_8qZ",
        "outputId": "fdc1c651-1fc9-4339-bc30-6c8cc164d280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on it!\n",
            "Successfully opened the URL!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing the html contents of the main page\n",
        "soup = beautifulsoap.BeautifulSoup(request.text, \"html.parser\")\n",
        "# finding the links to all departments\n",
        "departments = soup.find_all(class_ =  \"mt-5 btn btn-nu white\")\n",
        "# finding the department names from the above links\n",
        "department_names = [dep.text for dep in departments]\n",
        "# extracting the url for each department page\n",
        "department_url= [dep.get(\"href\") for dep in departments]"
      ],
      "metadata": {
        "id": "trFDIScIEW-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7_4ZjilHlyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # first of all finding the FSC though id\n",
        "def all_Information(request, myID):\n",
        "    fsc = soup.find(id= myID)\n",
        "    # getting the name of the emplyees in FSC through text centre\n",
        "    fsc_employee_id = fsc.find_all(\"h5\",\"text-center\")\n",
        "    # makes a list of names and puts in text from fsc_emplyee_id\n",
        "    name = [i.text for i in fsc_employee_id]\n",
        "\n",
        "    # now finding the tag that contains designation\n",
        "    desig = fsc.find_all('p', class_=\"small text-center font-italic\")\n",
        "    # making a list of all the designations and slpits them from where they ontain a break\n",
        "    designation = [designation.text.strip().split('\\n') for designation in desig]\n",
        "    temp = []\n",
        "    # simply just appending in a empty list\n",
        "    for i in designation:\n",
        "      temp.append(i[0])\n",
        "\n",
        "    #  Now checking if faculy is approved by hec or not\n",
        "    approval = [designation.find(\"br\") != -1 for designation in desig]\n",
        "    # appending true in temp1 if the loop comes to a br\n",
        "    temp1= []\n",
        "    for i in range(len(desig)):\n",
        "      if(desig[i].find('br')):\n",
        "        temp1.append(True)\n",
        "      else:\n",
        "        temp1.append(False)\n",
        "\n",
        "    #  now finding the email through the tags and the class\n",
        "    email = fsc.find_all('p', class_ = \"mb-0 text-center\")\n",
        "    #  simply going through all the emails and placing them in a list\n",
        "    email = [i.text for i in email]\n",
        "\n",
        "    #  now we find the department through h1 tag\n",
        "    dept = fsc.find('h1')\n",
        "\n",
        "    # finding the images through the tags and then placing them in a list\n",
        "    my_img = fsc.find_all(\"img\", class_ = \"card-img-top rounded-circle mt-3 mb-0 d-block mx-auto\")\n",
        "    img = [img['src'] for img in my_img]\n",
        "\n",
        "    my_id = fsc.find_all(\"a\",\"faculty-link\")# Geting the ID of Facuty Member\n",
        "    ids = [link.get(\"href\").split('/')[3] for link in my_id]\n",
        "     # just organizing as a dataframe with id giving as range of total no of employees\n",
        "    whole_data = pd.DataFrame({'ID' : ids,\n",
        "                                            \"Name \" : name,\n",
        "                                            'Designation': temp,\n",
        "                                            \"HEC Approval\": temp1,\n",
        "                                            \"Department\": dept,\n",
        "                                            \"Email\": email,\n",
        "                                            \"Image\": img})\n",
        "    #  turning the type to .csv according to the departement\n",
        "    whole_data.to_csv(myID + '.csv', index = False)\n",
        "    return whole_data\n",
        "listt = [\"fsc\", \"ee\", \"cv\", \"ss\", \"fsm\"]\n",
        "for i in listt:\n",
        "    all_Information(request, i)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1Y9y3U2GMUm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def some_Information(request, myID):\n",
        "    # First loading the file as CSV\n",
        "    info = pd.read_csv(myID + '.csv')\n",
        "    # Getting the IDs of the faculty members\n",
        "    ids = info['ID']\n",
        "    # Creating a list of all the URLs of the faculty members\n",
        "    URLs = [f\"https://lhr.nu.edu.pk/{myID}/facultyProfile/\" + str(i) for i in ids]\n",
        "    # Creating temp and empty lists\n",
        "    highestEducation = []\n",
        "    extension = []\n",
        "    # Looping through the URLS\n",
        "    for url in URLs:\n",
        "        # Getting the URL\n",
        "        req = requests.get(url)\n",
        "        # Getting the contents of the page of URL\n",
        "        soup1 = BeautifulSoup(req.content, \"html.parser\")\n",
        "        # Finding the education of the faculty members\n",
        "        education = soup1.find(\"div\", class_=\"col-lg-8 col-md-6 col-sm-12 text-justify\")\n",
        "        if education:\n",
        "            edu_items = education.find_all('li')  # Get all list items\n",
        "            highestEducation.append([item.text for item in edu_items])  # Collect all items\n",
        "        else:\n",
        "            highestEducation.append(None)\n",
        "        # Finding extension\n",
        "        ext = soup1.find(\"span\", class_=\"small\")\n",
        "        if ext:\n",
        "           extension_text = ext.text.split(\":\")[1].strip() if \":\" in ext.text else None\n",
        "           extension.append(extension_text)\n",
        "        else:\n",
        "            extension.append(None)\n",
        "    # Creating the DataFrame\n",
        "    whole_Data = pd.DataFrame({'ID': ids, \"Highest Education\": highestEducation, \"Extension\": extension})\n",
        "    # Saving to CSV according to the department\n",
        "    whole_Data.to_csv(myID + '2.csv', index=False)\n",
        "    return whole_Data\n",
        "# Call the function with appropriate parameters\n",
        "ids = [\"fsc\", \"ee\", \"cv\", \"ss\", \"fsm\"]\n",
        "for id in ids:\n",
        "  some_Information(request, id)\n"
      ],
      "metadata": {
        "id": "Kf6nRVQDnxQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating temp for storing all the data\n",
        "some_data = {}\n",
        "some_more_data = {}\n",
        "def some_More_Information(request, myID):\n",
        "  # reading from the first csv file and then storing in temp\n",
        "  data = pd.read_csv(myID + \".csv\")\n",
        "  some_data[myID] = data\n",
        "  # reading from the second csv file and then stroing in temp2\n",
        "  data2 = pd.read_csv(myID + \"2.csv\")\n",
        "  some_more_data[myID] = data2\n",
        "  # merging data based on id column and with left join\n",
        "  some_data[myID] = pd.merge(some_data[myID], some_more_data[myID], on = 'ID', how = 'left')\n",
        "\n",
        "  # Simple concatination while ignoring indexes\n",
        "  THIS_IS_WHOLE_DATA = pd.concat(some_data.values(), ignore_index=True)\n",
        "  # converting to a new csv file\n",
        "  THIS_IS_WHOLE_DATA.to_csv(\"fast_lahore_faculty.csv\", index = False)\n",
        "  return THIS_IS_WHOLE_DATA\n",
        "ids = [\"fsc\", \"ee\",\"cv\", \"ss\", \"fsm\"]\n",
        "for id in ids:\n",
        "  some_More_Information(request, id)"
      ],
      "metadata": {
        "id": "Z8BTsafS_h8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}