

NU Faculty Web Scraper

This project scrapes faculty data from NU Lahore Campus (http://lhr.nu.edu.pk/faculty/) using Python. It extracts faculty details like names, designations, HEC approval status, department, email addresses, and profile images from various departments and exports the results to structured CSV files.
ğŸ“‘ Table of Contents
âœ¨ Features
ğŸ“¦ Requirements
âš™ï¸ Setup Instructions
ğŸš€ How to Run
ğŸ“ Output
ğŸ“‚ Project Structure
ğŸ“œ License
âœ¨ Features
- Scrapes all departments from the NU faculty page dynamically.
- Extracts:
  - Faculty Name
  - Designation
  - HEC Approval status
  - Department
  - Email address
  - Profile image URL
  - Faculty ID
- Exports data to individual CSV files for each department.
ğŸ“¦ Requirements
Python 3.7+
Jupyter Notebook (optional)
Libraries:
- requests
- beautifulsoup4
- pandas
Install all dependencies using:
pip install -r requirements.txt
âš™ï¸ Setup Instructions
1. Clone this repository:
git clone https://github.com/yourusername/nu-faculty-scraper.git
cd nu-faculty-scraper
2. Install required Python packages:
pip install requests beautifulsoup4 pandas
ğŸš€ How to Run
Run the Jupyter Notebook using:
jupyter notebook WebScarping.ipynb
Or convert to a script and run:
jupyter nbconvert --to script WebScarping.ipynb
python WebScarping.py
ğŸ“ Output
The notebook will create CSV files for each department, e.g.:
fsc.csv
ee.csv
cv.csv
ss.csv
fsm.csv
Each CSV file contains structured faculty data with fields like:
- ID
- Name
- Designation
- HEC Approval (True/False)
- Department
- Email
- Image URL
ğŸ“‚ Project Structure
nu-faculty-scraper/
â”œâ”€â”€ WebScarping.ipynb       # Main notebook
â”œâ”€â”€ *.csv                   # Output files for each department
â”œâ”€â”€ requirements.txt        # List of dependencies
â””â”€â”€ README.md               # You're here!
ğŸ“œ License
This project is licensed under the MIT License.
ğŸ‘¨â€ğŸ’» Built with â¤ï¸ by M. Azib Naeem
